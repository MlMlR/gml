{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Trainingcenter\n",
    "\n",
    "## Supervised Learning - Assignment 1 : Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 - Regression Entry Questions\n",
    "\n",
    "1. What is a linear approach for a dataset $(y, X)$?\n",
    "    - It assumes that the relationship between the dependent variable $y$ and the independent variables (features) $X$ can be represented by a linear function.\n",
    "    - y = β0 + β1x1 + ... + βpxp + ε\n",
    "\n",
    "2. What is the Ordinary Least Squares (OLS) cost function of a linear approach?\n",
    "\n",
    "\n",
    "3. What is its gradient?\n",
    "\n",
    "\n",
    "4. How can categorical variables be used as features in a regression problem?\n",
    "\n",
    "\n",
    "5. Write pseudocode for the Gradient Descent algorithm for the OLS cost function.\n",
    "    - nitialize the parameters $\\theta$ with small random values\n",
    "    - Set the learning rate $\\alpha$\n",
    "    - Repeat until convergence \n",
    "        - Calculate the predicted values: $h_\\theta(x^{(i)})$\n",
    "        - Update the parameters: $\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_j^{(i)}$ for all $j$\n",
    "    \n",
    "\n",
    "\n",
    "6. What are possible metrics for evaluating a regression?\n",
    "\n",
    "\n",
    "7. What forms of regularization do you know for linear regression? How do they differ?\n",
    "\n",
    "\n",
    "8. What is the purpose of regularization?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 2 - Entwicklungsdatensatz\n",
    "\n",
    "Erzeuge einen Testdatensatz $(x, y)$ für ein einfaches lineares Regressionsproblem mit 1000 Datenpunkten unter Verwendung der Koeffizienten $\\beta_0 = 6.0$, $\\beta_1 = 1.5 $, $\\sigma=0.1$ (Varianz der Fehlervariablen) und normalverteilten $x_i$ mit Mittelwert $\\mu = 0.5$.  \n",
    "\n",
    "Schreibe dazu eine Funktion, mit welcher du einfach weitere Testdatensätze erzeugen kannst.  \n",
    "Verwende `np.random` zur Erzeugung der Zufallszahlen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_dataset(n_points, beta_0, beta_1, sigma, mu):\n",
    "    x = np.random.normal(mu, 1, n_points)\n",
    "    eps = np.random.normal(0, sigma, n_points)\n",
    "    y = beta_0 + beta_1 * x + eps\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "# Generate the dataset\n",
    "x, y = generate_dataset(n_points=1000, beta_0=6.0, beta_1=1.5, sigma=0.1, mu=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 3 - Lineare Regression Gradient Descent\n",
    "\n",
    "Scikit-learn hat eine sehr einfache und in der Zwischenzeit von vielen Bibliotheken übernommene API für Machine Learning-Methoden. Lies dazu das Paper im Ressources GitLab Repo von Buitinck et al. 2013.\n",
    "\n",
    "Komplettiere die folgende Klasse dann so, dass wir damit mit Gradient Descent und der OLS-Kostenfunktion einen linearen Ansatz auf einen Datensatz 'fitten' können.\n",
    "\n",
    "Führe dann auf deinem Testdatensatz eine erste lineare Regression durch. Spiele allenfalls mit den Model-Parametern bis deine Lösung konvergiert.  \n",
    "\n",
    "Zeichne den Wert der Kostenfunktion als Funktion der Iterationsschritte.  \n",
    "\n",
    "Plotte schliesslich die Daten als Punkte und darüber die Regressionsgerade.  \n",
    "\n",
    "Füge eine Methode `.score(X, y)` der Klasse hinzu welche das Bestimmtheitsmass $R^2$ für einen Datensatz $(X, y)$ berechnet und berechne das Bestimmtheismass deines linearen Modells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionGradientDescent(object):\n",
    "    '''\n",
    "    This class implements a gradient descent optimizer for the ordinary least squares (OLS) cost function. \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, init_coef=(0., 0.), epsilon=0.0000001, maxsteps=1000,\n",
    "                 stepsize=0.001):\n",
    "        '''\n",
    "        This class implements a gradient descent optimizer for the least squares cost  \n",
    "        '''\n",
    "        self.init_coef = init_coef\n",
    "        self.coef_ = np.array(self.init_coef)\n",
    "        self._nsteps = 0\n",
    "        self.stepsize = stepsize\n",
    "        self.epsilon = epsilon\n",
    "        self.maxsteps = maxsteps\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, verbose=False):\n",
    "        '''\n",
    "        Fits the coefficients beta of a linear regression problem\n",
    "        to the dataset (X, y) .\n",
    "        '''      \n",
    "        ### BEGIN SOLUTION\n",
    "        \n",
    "        # Gradient Descent\n",
    "        \n",
    "        ### END\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Predicts output values y.\n",
    "        '''\n",
    "        return X.dot(self.coef_)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def least_squares_gradient(coef, X, y):\n",
    "        '''\n",
    "        Calculates the least squares gradient at position coef\n",
    "        for the dataset (X, y).\n",
    "        '''\n",
    "        ### BEGIN SOLUTION\n",
    "        \n",
    "        \n",
    "        ### END SOLUTION\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "    \n",
    "    def score(self, X, y):\n",
    "        '''Returns R^2.\n",
    "        '''\n",
    "        ### BEGIN SOLUTION\n",
    "        \n",
    "        ### END SOLUTION\n",
    "        \n",
    "        return Rsquare\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def cost_function(X, y, beta):\n",
    "        '''\n",
    "        Calculates the value of a OLS cost function.\n",
    "        '''\n",
    "        ### BEGIN SOLUTION\n",
    "      \n",
    "        ### END SOLUTION\n",
    "        \n",
    "        return OLScost\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 4 - Annahmen eines linearen Modells\n",
    "\n",
    "1. Was sind die Annahmen eines linearen Modells?\n",
    "2. Wie können sie grafisch untersucht werden?\n",
    "\n",
    "(Das Skript von Stahel bietet hierzu eine tiefgehende Auseinandersetzung.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 5 - Koeffizienten Ebene\n",
    "\n",
    "Verwende den folgenden Code, um für die zwei Koeffizienten $(\\beta_0, \\beta_1)$ des linearen Ansatzes für den Entwicklungsdatensatz die Werte der Kostenfunktion als Äquipotentiallinien zu zeichnen.  \n",
    "\n",
    "Zeichne dann den Weg der von deinem Gradient Descent-Algorithmus von einem beliebigen Startort innerhalb des Plots weg beschritten wird als rote Kurve in den Plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_contour_plot(X, y, betalims=(-10., 10.), resolution=.2, ax=None):\n",
    "\n",
    "    # create meshgrid\n",
    "    xx1, xx2 = np.meshgrid(np.arange(betalims[0], betalims[1], resolution),\n",
    "                         np.arange(betalims[0], betalims[1], resolution))\n",
    "\n",
    "    Z = []\n",
    "    for b0, b1 in zip(xx1.ravel(), xx2.ravel()):\n",
    "        Z.append(LinearRegressionGradientDescent.cost_function(X, y, np.array([b0, b1])))\n",
    "\n",
    "    Z = np.array(Z).reshape(xx1.shape)\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    else:\n",
    "        fig = None\n",
    "\n",
    "    ax.contour(xx1, xx2, Z, levels=np.linspace(Z.min(), Z.max(), 20))\n",
    "    ax.set_ylabel(r'$\\beta_1$')\n",
    "    ax.set_xlabel(r'$\\beta_0$')\n",
    "    \n",
    "    if fig is not None:\n",
    "        return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 6 - Diabetes Datensatz, Model Selection\n",
    "\n",
    "Lade den 'diabetes'-Datensatz (`data/diabetes.csv`) in ein pandas DataFrame und gib die ersten 10 Zeilen aus.  \n",
    "\n",
    "Visualisiere den Datensatz.  \n",
    "\n",
    "Wir wollen den Output-Wert $Y$, eine Messgrösse die mit Diabetes assoziiert ist, vorhersagen.\n",
    "Entwickle dazu drei Modelle durch Verwendung der entsprechenden scikit-learn Estimators: 1. ein lineares Modell, 2. ein Ridge-Regression Modell, 3. ein Lasso-Modell.\n",
    "\n",
    "Wie kannst du einen optimalen Wert für die Regularisierungsstärke $\\alpha$ in den regularisierten Modellen bestimmen? Wie kannst du die Vohersagekraft der Modelle sinnvoll messen?\n",
    "\n",
    "Wie unterscheiden sie sich hinsichtlich $R^2$ und MAE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
